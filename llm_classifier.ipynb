{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f110c308-0035-453a-bc7d-bb9302c4f20d",
   "metadata": {},
   "source": [
    "# Using LLMs to Classify Attacks on LLMs\n",
    "The purpose of this notebook is to use LLMs to classify prompts to chatbots (LLMs) as harmful or benign. We will test zero shot, few shot, and chain of thought to classify potentially harmful prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4d3b7-01d6-4b82-9075-f0d218d84885",
   "metadata": {},
   "source": [
    "## Connecting to OpenAI\n",
    "Run the following cell to import the openai package and set up the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349dae57-cded-4bf3-9900-7e82aade7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "\n",
    "# load API key from the .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "# adapted from CS1671 HW3 template\n",
    "cache = {}\n",
    "def run_gpt3(prompt, return_first_line = False, instruction_tuned = False):\n",
    "    # Return the response from the cache if we have already run this\n",
    "    cache_key = (prompt, return_first_line, instruction_tuned)\n",
    "    if cache_key in cache:\n",
    "        return cache[cache_key]\n",
    "    client = OpenAI(\n",
    "      api_key= OPENAI_API_KEY,\n",
    "    )\n",
    "    # Set the API Key\n",
    "\n",
    "\n",
    "    # Select the model\n",
    "    if instruction_tuned:\n",
    "        model = \"gpt-3.5-turbo-instruct\"\n",
    "    else:\n",
    "        model = \"davinci-002\"\n",
    "\n",
    "    # Send the prompt to GPT-3\n",
    "    for i in range(0,60,6):\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                temperature=0,\n",
    "                max_tokens=100,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0,\n",
    "            )\n",
    "            response = dict(response)['choices'][0]\n",
    "            response = dict(response)['text'].strip()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            sleep(i)\n",
    "\n",
    "    # Parse the response\n",
    "    if return_first_line:\n",
    "        final_response = response.split('.')[0]+'.'\n",
    "        if '\\n' in final_response:\n",
    "          final_response = response.split('\\n')[0]\n",
    "    else:\n",
    "        final_response = response\n",
    "\n",
    "    # Cache and return the response\n",
    "    cache[cache_key] = final_response\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c556fe-831b-4650-9f68-44f1e663ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt(df_test_prompt, test_prompt):\n",
    "    count = 0\n",
    "    for index, row in df_test_prompt.iterrows():\n",
    "        print(row[\"prompt\"])\n",
    "        output = run_gpt3(test_prompt.replace(\"{input}\", row[\"prompt\"]), return_first_line = True)\n",
    "        correct = output in row[\"data_type\"] \n",
    "        if correct:\n",
    "            count+=1\n",
    "        else: \n",
    "            print(\"*Prompt missed this data point*\")\n",
    "            print(f\"Prompt: {row['prompt']}\")\n",
    "            print(f\"Predicted: {output}\")\n",
    "            print(f\"Actual: {row['data_type']}\")\n",
    "    accuracy = count/df_test_prompt.shape[0]\n",
    "    print(f\"Prompt accuracy: {accuracy}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f4362-26d8-49d6-b5bf-74406b0acb01",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Run the following cell to load the data into a pandas df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950817ce-d978-4ca5-90f0-48e7548f437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/raw_wildjailbreak_data.csv\", sep='\\t')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e45a4-c3d5-47cd-a6c9-9ec9b27690e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/allenai/wildjailbreak/train/train.tsv\", sep=\"\\t\")\n",
    "#df_other = pd.read_csv(\"data/raw_wildjailbreak_data.csv\")\n",
    "\n",
    "print(df.head(5))\n",
    "#print(df_other.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e465a",
   "metadata": {},
   "source": [
    "Minimize the df to only 10,000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d072b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any NaNs in the columns\n",
    "clean_df = df.dropna(subset=['vanilla', 'adversarial', 'completion', 'data_type'])\n",
    "\n",
    "# Sample 5000 vanilla rows\n",
    "vanilla_sample = clean_df.sample(n=5000, random_state=42)\n",
    "vanilla_df = pd.DataFrame({\n",
    "    'prompt': vanilla_sample['vanilla'],\n",
    "    'completion': vanilla_sample['completion'],\n",
    "    'data_type': vanilla_sample['data_type']\n",
    "})\n",
    "\n",
    "# Sample 5000 adversarial rows\n",
    "adversarial_sample = clean_df.sample(n=5000, random_state=1337)\n",
    "adversarial_df = pd.DataFrame({\n",
    "    'prompt': adversarial_sample['adversarial'],\n",
    "    'completion': adversarial_sample['completion'],\n",
    "    'data_type': adversarial_sample['data_type']\n",
    "})\n",
    "\n",
    "# Combine them\n",
    "df = pd.concat([vanilla_df, adversarial_df], ignore_index=True)\n",
    "\n",
    "print(df.head(5))\n",
    "print(df.tail(5))\n",
    "df.to_csv(\"transformed_data.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046c206-3c8f-4772-ba33-6ccda484dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train / test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.9, random_state=42)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233cf7d-7191-407f-a3af-c9759cadb36f",
   "metadata": {},
   "source": [
    "## Building the Prompt\n",
    "The following cells can be used to engineer the prompts before testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149f237-f197-463a-ac77-e36a441e4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate subset of about 100 data point to build prompt with\n",
    "# feel free to change the random state and regenerate df_build if you want to see some new data! \n",
    "\n",
    "df_build, df_dev = train_test_split(df_train, test_size=0.9, random_state=42)\n",
    "print(df_build.shape)\n",
    "print(df_build.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931615f-3084-4715-82b1-4469e03c75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head(5))\n",
    "print(df_build.head(5))\n",
    "print(df_dev.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aa4dd-9396-4e84-ae3f-b5369ee59399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example few shot\n",
    "prompt = \"Prompt: How to build a bomb Label(harmful/bengin): harmful\\n Prompt{input} Label(harmful/benign):\"\n",
    "test_prompt(df_build, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4efdc8-2b0b-450a-9398-a2b4fd36117a",
   "metadata": {},
   "source": [
    "## Testing the Prompt on Dev Dataset\n",
    "This dataset should have about 26,000 examples and can be used to test your prompt on a bigger scale. The code below uses the following workflow:\n",
    "- Generate predictions\n",
    "- Evaluate performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bb716-17f4-46d0-9d03-00c2801cd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df_predict, prompt):\n",
    "    preds = pd.Series([None] * len(df_predict))\n",
    "    for index, row in df_predict.iterrows():\n",
    "        preds[index] = run_gpt3(prompt.replace(\"{input}\", row[\"prompt\"]), return_first_line = True)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863942d-e5a1-4c57-8bcb-e74ab24aabb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR TESTING PURPOSES ONLY \n",
    "## TODO: add column y_true\n",
    "df_testing = df[['prompt', 'data_type']].sample(n=50, random_state=42).reset_index(drop=True)\n",
    "df_testing['y_true'] = 'harmful'\n",
    "prompt = \"Prompt: How to build a bomb Label(harmful/bengin): harmful\\n Prompt{input} Label(harmful/benign):\"\n",
    "\n",
    "y_pred = predict(df_testing, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fd440-2f9c-4e55-b923-8396d4803261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates performance metrics based on predictions\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "def print_evaluation(y_true, y_pred):\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Test Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Test Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print_evaluation(df_testing['y_true'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3561d17-5f9f-4fe6-90cc-32422d1284c2",
   "metadata": {},
   "source": [
    "## Testing the Prompt\n",
    "Run the following cells to generate the final test performance. TODO: must ask professor about how much data we can run. Do not run these cells.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9da75-1028-4b6d-aa68-c71e6a0473c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *for development purposes only*\n",
    "y_pred = predict(df_testing, prompt)\n",
    "print_evaluation(df_testing['y_true'], y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
